{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool for scraping the publication info of the team from Google Scholar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scholarly import scholarly, ProxyGenerator\n",
    "from os import path, getcwd, chdir, listdir\n",
    "import markdown\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some general variables, may need to be adapted upon structural changes to the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the folder that stores the information of all team members\n",
    "people_folder = path.join(getcwd(), 'people')\n",
    "# The name of the meta tag, which stores the necessary information about the team member for the search query.\n",
    "meta_name_att = 'name'\n",
    "meta_affiliation_att = 'affiliation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up Proxies for the scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "pg = ProxyGenerator()\n",
    "\n",
    "scholarly.use_proxy(pg, pg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the folders of each member, a Markdown file is stored containing Meta information of the member. We want to access the meta information in order to build up the search queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abdullahi Ali\n",
      "start processing 2 publications\n",
      "begin processing publication #0\n",
      "Bodo RÃ¼ckauer\n",
      "no information found for the author, proceeding to the next\n",
      "Burcu KÃ¼Ã§Ã¼koÄŸlu\n",
      "no information found for the author, proceeding to the next\n",
      "Elsbeth van Dam\n",
      "no information found for the author, proceeding to the next\n",
      "Feyisayo Olalere\n",
      "no information found for the author, proceeding to the next\n",
      "Gianluigi Silvestri\n",
      "start processing 5 publications\n",
      "begin processing publication #0\n",
      "Jaap de Ruyter van Steveninck\n",
      "start processing 5 publications\n",
      "begin processing publication #0\n",
      "JesÃºs G. Fernandez\n",
      "no information found for the author, proceeding to the next\n",
      "Julia Berezutskaya\n",
      "start processing 15 publications\n",
      "begin processing publication #0\n",
      "begin processing publication #10\n",
      "Justus Hubotter\n",
      "no information found for the author, proceeding to the next\n",
      "Kees Koenders\n",
      "no information found for the author, proceeding to the next\n",
      "Kiki van der Heijden\n",
      "start processing 11 publications\n",
      "begin processing publication #0\n",
      "begin processing publication #10\n",
      "Lex Dingemans\n",
      "no information found for the author, proceeding to the next\n",
      "Luca Ambrogioni\n",
      "start processing 45 publications\n",
      "begin processing publication #0\n",
      "begin processing publication #10\n",
      "begin processing publication #20\n",
      "begin processing publication #30\n",
      "begin processing publication #40\n",
      "Marcel van Gerven\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__enter__\n",
      "Fetching only the top 20 coauthors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start processing 280 publications\n",
      "begin processing publication #0\n",
      "begin processing publication #10\n",
      "begin processing publication #20\n",
      "begin processing publication #30\n",
      "begin processing publication #40\n",
      "begin processing publication #50\n",
      "begin processing publication #60\n",
      "begin processing publication #70\n",
      "begin processing publication #80\n",
      "begin processing publication #90\n",
      "begin processing publication #100\n",
      "begin processing publication #110\n",
      "begin processing publication #120\n",
      "begin processing publication #130\n",
      "begin processing publication #140\n",
      "begin processing publication #150\n",
      "begin processing publication #160\n",
      "begin processing publication #170\n",
      "begin processing publication #180\n",
      "begin processing publication #190\n",
      "begin processing publication #200\n",
      "begin processing publication #210\n",
      "begin processing publication #220\n",
      "begin processing publication #230\n",
      "begin processing publication #240\n",
      "begin processing publication #250\n",
      "begin processing publication #260\n",
      "begin processing publication #270\n",
      "Michelle Appel\n",
      "no information found for the author, proceeding to the next\n",
      "Nasir Ahmad\n",
      "start processing 14 publications\n",
      "begin processing publication #0\n",
      "begin processing publication #10\n",
      "Renato Farinha Duarte\n",
      "no information found for the author, proceeding to the next\n",
      "Sander Dalm\n",
      "no information found for the author, proceeding to the next\n",
      "Sander Keemink\n",
      "no information found for the author, proceeding to the next\n",
      "Tal Kachman\n",
      "no information found for the author, proceeding to the next\n"
     ]
    }
   ],
   "source": [
    "# The 'people' folder holds a subfolder for each member of the team\n",
    "subfolders = listdir(people_folder)\n",
    "\n",
    "# Set up a Markdown parser\n",
    "md_parser = markdown.Markdown(extensions=['meta'])\n",
    "\n",
    "# Iterate through the folders of each member\n",
    "for member_folder in subfolders:\n",
    "  # Complete the path to the folder of the member we currently process\n",
    "  full_path = path.join(people_folder, member_folder)\n",
    "  # Build the path to the Markdown file in the current subfolder\n",
    "  md_path = path.join(full_path, [filename for filename in listdir(full_path) if '.md' in filename ][0])\n",
    "  \n",
    "  # Read in the Markdown file\n",
    "  with open(md_path, 'r') as md_file:\n",
    "    text = md_file.read()\n",
    "    # Make use of the Markdown Parser to access the Meta information that is stored in the file\n",
    "    md_parser.convert(text)\n",
    "    # Retrieve the Metadata of the author\n",
    "    author_name = md_parser.Meta[meta_name_att][0].replace('\"', '')\n",
    "    author_affiliation = md_parser.Meta[meta_affiliation_att][0].replace('\"', '')\n",
    "    print(author_name)\n",
    "\n",
    "  # Use scholarly to build up a search query for the team member\n",
    "  author_query = scholarly.search_author(' '.join([author_name, author_affiliation]))\n",
    "  \n",
    "  # Execute the search query to fill up information about the team member\n",
    "  try:\n",
    "    author = scholarly.fill(next(author_query))\n",
    "    \n",
    "    # Collect information of all publications of the team member\n",
    "    publications = []\n",
    "    num_pubs = len(author['publications'])\n",
    "    print(f'start processing {num_pubs} publications')\n",
    "    # Iterate over each entry in the publication list\n",
    "    # NOTE: The entries here are not filled up yet by scholarly, but simple placeholders containing some meta info\n",
    "    # that can be used for further querying\n",
    "    for idx, pub_query in enumerate(author['publications']):\n",
    "      if idx % 10 == 0:\n",
    "        print(f'begin processing publication #{idx}')\n",
    "      # use the placeholder query to retrieve the actual publication info from Google Scholar\n",
    "      publications.append(scholarly.fill(pub_query))\n",
    "    \n",
    "    # Save the collected publication info appropriately\n",
    "    with open(path.join(full_path, 'publications.json'), 'w', encoding='utf-8') as f:\n",
    "      json.dump(publications, f, ensure_ascii=False, indent=4)\n",
    "      \n",
    "  except StopIteration:\n",
    "    print('no information found for the author, proceeding to the next')\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "206bd255909268dc9ca464f38b020e196356f9ed46c9aec7b5fddb8ea85fd10d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

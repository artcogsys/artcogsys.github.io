[
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Searching relevant variable subsets in complex systems using k-means PSO",
            "pub_year": 2017,
            "author": "Gianluigi Silvestri and Laura Sani and Michele Amoretti and Riccardo Pecori and Emilio Vicari and Monica Mordonini and Stefano Cagnoni",
            "conference": "Italian Workshop on Artificial Life and Evolutionary Computation",
            "pages": "308-321",
            "publisher": "Springer, Cham",
            "abstract": "The Relevance Index method has been shown to be effective in identifying Relevant Sets in complex systems, i.e., variable sub-sets that exhibit a coordinated behavior, along with a clear independence from the remaining variables. The need for computing the Relevance Index for each possible variable sub-set makes such a computation unfeasible, as the size of the system increases. Because of this, smart search methods are needed to analyze large-size systems using such an approach. Niching metaheuristics provide an effective solution to this problem, as they join search capabilities to good exploration properties, which allow them to explore different regions of the search space in parallel and converge onto several local/global minima.In this paper, we describe the application of a niching metaheuristic, K-means PSO, to a set of complex systems of different size, comparing, when possible, its â€¦"
        },
        "filled": true,
        "author_pub_id": "BKRLVogAAAAJ:u-x6o8ySG0sC",
        "num_citations": 14,
        "citedby_url": "/scholar?cites=5727892983920118497",
        "cites_id": [
            "5727892983920118497"
        ],
        "pub_url": "https://link.springer.com/chapter/10.1007/978-3-319-78658-2_23",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:4coCOlKQfU8J:scholar.google.com/",
        "cites_per_year": {
            "2018": 4,
            "2019": 4,
            "2020": 2,
            "2021": 4
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Automatic variational inference with cascading flows",
            "pub_year": 2021,
            "author": "Luca Ambrogioni and Gianluigi Silvestri and Marcel Van Gerven",
            "conference": "International Conference on Machine Learning",
            "pages": "254-263",
            "publisher": "PMLR",
            "abstract": "The automation of probabilistic reasoning is one of the primary aims of machine learning. Recently, the confluence of variational inference and deep learning has led to powerful and flexible automatic inference methods that can be trained by stochastic gradient descent. In particular, normalizing flows are highly parameterized deep models that can fit arbitrarily complex posterior densities. However, normalizing flows struggle in highly structured probabilistic programs as they need to relearn the forward-pass of the program. Automatic structured variational inference (ASVI) remedies this problem by constructing variational programs that embed the forward-pass. Here, we combine the flexibility of normalizing flows and the prior-embedding property of ASVI in a new family of variational programs, which we named cascading flows. A cascading flows program interposes a newly designed highway flow architecture in between the conditional distributions of the prior program such as to steer it toward the observed data. These programs can be constructed automatically from an input probabilistic program and can also be amortized automatically. We evaluate the performance of the new variational programs in a series of structured inference problems. We find that cascading flows have much higher performance than both normalizing flows and ASVI in a large set of structured inference problems."
        },
        "filled": true,
        "author_pub_id": "BKRLVogAAAAJ:qjMakFHDy7sC",
        "num_citations": 4,
        "citedby_url": "/scholar?cites=16991650093300670925",
        "cites_id": [
            "16991650093300670925"
        ],
        "pub_url": "http://proceedings.mlr.press/v139/ambrogioni21a.html",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:zWHr9rt3zusJ:scholar.google.com/",
        "cites_per_year": {
            "2021": 4
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Embedded-model flows: Combining the inductive biases of model-free deep learning and explicit probabilistic modeling",
            "pub_year": 2022,
            "author": "Gianluigi Silvestri and Emily Fertig and Dave Moore and Luca Ambrogioni",
            "conference": "International Conference on Learning Representations",
            "abstract": "Normalizing flows have shown great success as general-purpose density estimators. However, many real world applications require the use of domain-specific knowledge, which normalizing flows cannot readily incorporate. We propose embedded-model flows (EMF), which alternate general-purpose transformations with structured layers that embed domain-specific inductive biases. These layers are automatically constructed by converting user-specified differentiable probabilistic models into equivalent bijective transformations. We also introduce gated structured layers, which allow bypassing the parts of the models that fail to capture the statistics of the data. We demonstrate that EMFs can be used to induce desirable properties such as multimodality, hierarchical coupling and continuity. Furthermore, we show that EMFs enable a high performance form of variational inference where the structure of the prior model is embedded in the variational architecture. In our experiments, we show that this approach outperforms state-of-the-art methods in common structured inference problems."
        },
        "filled": true,
        "author_pub_id": "BKRLVogAAAAJ:UeHWp8X0CEIC",
        "num_citations": 2,
        "citedby_url": "/scholar?cites=13875622113438069976",
        "cites_id": [
            "13875622113438069976"
        ],
        "pub_url": "https://arxiv.org/abs/2110.06021",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:2PQHxcsckMAJ:scholar.google.com/",
        "cites_per_year": {
            "2021": 1,
            "2022": 1
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Closing the gap: Exact maximum likelihood training of generative autoencoders using invertible layers",
            "pub_year": 2022,
            "author": "Gianluigi Silvestri and Daan Roos and Luca Ambrogioni",
            "journal": "arXiv preprint arXiv:2205.09546",
            "abstract": "In this work, we provide an exact likelihood alternative to the variational training of generative autoencoders. We show that VAE-style autoencoders can be constructed using invertible layers, which offer a tractable exact likelihood without the need for any regularization terms. This is achieved while leaving complete freedom in the choice of encoder, decoder and prior architectures, making our approach a drop-in replacement for the training of existing VAEs and VAE-style models. We refer to the resulting models as Autoencoders within Flows (AEF), since the encoder, decoder and prior are defined as individual layers of an overall invertible architecture. We show that the approach results in strikingly higher performance than architecturally equivalent VAEs in term of log-likelihood, sample quality and denoising performance. In a broad sense, the main ambition of this work is to close the gap between the normalizing flow and autoencoder literature under the common framework of invertibility and exact maximum likelihood."
        },
        "filled": true,
        "author_pub_id": "BKRLVogAAAAJ:zYLM7Y9cAGgC",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2205.09546",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:1y8f7EsJbDUJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "One-Shot Neural Architecture Search for Deep Multi-Task Learning in Computer Vision",
            "pub_year": 2020,
            "author": "Gianluigi Silvestri",
            "abstract": "In this work, a neural architecture search algorithm for multi-task learning is proposed. Given any dataset and tasks group, the method aims to find the optimal way of sharing layers among tasks in convolutional neural networks. A search space suited to multi-task learning is designed, and a novel strategy to rank different Pareto optimal solutions is developed. The core of the algorithm is an adaptation of a state-of-the-art neural architecture search strategy. Experimental results on the Cityscapes dataset, on the tasks of semantic segmentation and depth estimation, do not provide the expected results. Despite the lack of stable results, this work lays down the fundamentals to further develop novel multi-task neural architecture search methods."
        },
        "filled": true,
        "author_pub_id": "BKRLVogAAAAJ:9yKSN-GCB0IC",
        "num_citations": 0,
        "pub_url": "https://www.diva-portal.org/smash/record.jsf?pid=diva2:1472082",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:CE-XeolEDIIJ:scholar.google.com/",
        "cites_per_year": {}
    }
]